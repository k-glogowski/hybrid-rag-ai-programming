"""
Streamlit GUI do chatu z workflow RAG (Docker docs).
Technical details (peÅ‚ny stan flow) w accordion "details".
"""
import logging
import streamlit as st

logging.basicConfig(level=logging.INFO, format="%(name)s - %(levelname)s - %(message)s")
from langchain_core.messages import HumanMessage

from build_index import delete_index, rebuild_index
from workflow import get_graph, clear_graph_cache


def message_to_dict(msg):
    """Konwersja wiadomoÅ›ci LangChain do sÅ‚ownika do wyÅ›wietlenia."""
    d = {
        "type": type(msg).__name__,
        "content": getattr(msg, "content", None) or "",
    }
    if hasattr(msg, "tool_calls") and msg.tool_calls:
        d["tool_calls"] = msg.tool_calls
    if hasattr(msg, "additional_kwargs") and msg.additional_kwargs:
        d["additional_kwargs"] = msg.additional_kwargs
    return d


def state_to_display(state):
    """Konwersja stanu workflow do czytelnej reprezentacji (historia flow)."""
    if not state or "messages" not in state:
        return {}
    return {
        "messages": [message_to_dict(m) for m in state["messages"]],
    }


st.set_page_config(page_title="Chat â€“ Docker docs RAG", page_icon="ğŸ³", layout="wide")
st.title("ğŸ³ Chat z dokumentacjÄ… Docker")
st.caption("Zadaj pytanie â€“ workflow moÅ¼e wyszukaÄ‡ fragmenty dokumentacji i odpowiedzieÄ‡.")

# Toolbox (sidebar) â€“ ustawienia retrievera i indeksu Chroma
with st.sidebar:
    st.header("ğŸ”§ Toolbox")

    st.subheader("Wyszukiwanie")
    retriever_k = st.slider(
        "Liczba chunkÃ³w z retrievera",
        min_value=1,
        max_value=20,
        value=4,
        step=1,
        help="Ile fragmentÃ³w dokumentacji zwraca wyszukiwarka przy jednym zapytaniu.",
    )

    st.subheader("Indeks Chroma (chunki)")
    chunk_size = st.number_input(
        "DÅ‚ugoÅ›Ä‡ chunka",
        min_value=100,
        max_value=2000,
        value=400,
        step=50,
        help="Rozmiar fragmentu tekstu przy dzieleniu dokumentÃ³w (znaki/tokeny).",
    )
    chunk_overlap = st.number_input(
        "NakÅ‚adka chunkÃ³w (overlap)",
        min_value=0,
        max_value=500,
        value=100,
        step=10,
        help="Ile znakÃ³w wspÃ³lnych miÄ™dzy sÄ…siednimi chunkami.",
    )

    col_del, col_rebuild = st.columns(2)
    with col_del:
        if st.button("ğŸ—‘ï¸ UsuÅ„ indeks", help="CzyÅ›ci dane indeksu z bazy Chroma (bez usuwania plikÃ³w)."):
            clear_graph_cache()  # zwolnij referencje do Chroma przed czyszczeniem
            removed = delete_index()
            if removed:
                st.success("Indeks usuniÄ™ty.")
            else:
                st.info("Brak indeksu do usuniÄ™cia.")
            st.rerun()
    with col_rebuild:
        if st.button("ğŸ”„ Przebuduj indeks", help="CzyÅ›ci dane indeksu i buduje od zera z powyÅ¼szymi parametrami chunkÃ³w (bez usuwania plikÃ³w)."):
            with st.spinner("PrzebudowujÄ™ indeks..."):
                clear_graph_cache()  # zwolnij referencje do Chroma przed czyszczeniem
                rebuild_index(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
            st.success("Indeks przebudowany.")
            st.rerun()

if "chat_messages" not in st.session_state:
    st.session_state.chat_messages = []

for msg in st.session_state.chat_messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])
        # Details (stan flow) przy kaÅ¼dej wiadomoÅ›ci asystenta
        if msg["role"] == "assistant" and msg.get("flow_state") is not None:
            with st.expander("details", expanded=False):
                st.caption("Technical details â€“ stan flow dla tej odpowiedzi")
                display_state = state_to_display(msg["flow_state"])
                st.json(display_state)

if prompt := st.chat_input("Twoje pytanie..."):
    st.session_state.chat_messages.append({"role": "user", "content": prompt})

    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("Przetwarzam..."):
            graph = get_graph(retriever_k=retriever_k)
            messages_for_graph = [HumanMessage(content=prompt)]
            result = graph.invoke({"messages": messages_for_graph})

        answer = result["messages"][-1].content
        st.markdown(answer)
        st.session_state.chat_messages.append({
            "role": "assistant",
            "content": answer,
            "flow_state": result,
        })
        # WyÅ›wietl details od razu dla nowej odpowiedzi
        with st.expander("details", expanded=False):
            st.caption("Technical details â€“ stan flow dla tej odpowiedzi")
            display_state = state_to_display(result)
            st.json(display_state)
